# *****************************************************************************
# ** DATE: 10/14/2020
# ** BY: FATEMEH SHAHSAVARI
# ** TITLE: SEQUESNTIAL FORWARD FEATURE SELECTION (SFS)
# ** DESCRIPTION: 
# ** - THIS CODE USES FORWARD SEARCH METHOD TO SELECT THE BEST PERFORMING FEATURES, BASED ON THE LINEAR DISCRIMINANT ANALYSIS (LDA) CLASSIFIER ACCURACY.
# ** - THE ORIGINAL DATASET (MATERIAL STACKING FAULT ENERGY) CAN BE ACCESSED HERE: https://braganeto.engr.tamu.edu/book-website/
# *****************************************************************************

# *******************************  LIBRARIES  *********************************
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn import metrics
###############################################################################

###############################################################################
# *********************************  CODE BODY ********************************  
#Load the original dataset. The dataset can be accessed here: https://braganeto.engr.tamu.edu/book-website/
#The pre-processing to remove zero components is from: https://braganeto.engr.tamu.edu/book-website/
dataPoints  = pd.read_table("Stacking_Fault_Energy_Dataset.txt")
f_org = dataPoints.columns[:-1]                # original features 
n_org = dataPoints.shape[0]                    # original number of training points
p_org = np.sum(dataPoints.iloc[:,:-1]>0)/n_org # fraction of nonzero components for each feature
f_drp = f_org[p_org<0.6]		             # features with less than 60% nonzero components
SFE1  = dataPoints.drop(f_drp,axis=1)          # drop those features
s_min = SFE1.min(axis=1)                     
SFE2  = SFE1[s_min!=0]                       # drop sample points with any zero values  
SFE   = SFE2[(SFE2.SFE<35)|(SFE2.SFE>45)]    # drop sample points with middle responses
n_samples = int(SFE.shape[0])
Y = SFE.SFE > 40 # if SFE > 40, Y =true , else Y=false
# so Y == True and ~Y==False which represent the labels

#Pick the first 80% of the sample points to be the training data and the remaining 20% to be the test data.
splitPoint = int(0.8*len(SFE))
trainSample = SFE[:splitPoint] 
testSample = SFE[splitPoint:]
#Two classes are defined: Class 0 contains the data with Stacking_Fault_Energy (SFE) > 40 and class 1 includes the data resulting SFE < 40.
train_X = trainSample.iloc[:,:-1].values 
train_Y = trainSample.iloc[:,-1]>40 
test_X = testSample.iloc[:,:-1].values 
test_Y = testSample.iloc[:,-1]>40

#Create the combinations of feature sets. 
def GenerateCombinations(initialList,featureNumbers): #'initialList' is the list of all available features. 'featureNumbers' is the number of features in a combination.
    finalList = []
    
    if featureNumbers == 1:
    
        for i in range (len(initialList)):
            finalList.append([initialList[i]])
        return finalList

    while len(initialList):
        temp = initialList.pop(0)
        initialList_Copy = initialList.copy()
        listLocal=GenerateCombinations(initialList_Copy,featureNumbers-1)
        featureNumbers_New=len(listLocal)
        
        for i in range(featureNumbers_New):
            listLocal[i].insert(0,temp)
        finalList +=listLocal
        
    return finalList

#Select the final combination with the best performance in each subset (1_feature subsets to 5_feature subsets).
#The performance is evaluated based on the accuracy of the Linear Discriminant Analysis (LDA) classifier. 
featureNames = ["C","N","Ni","Fe","Mn","Si","Cr"]
index = [0,1,2,3,4,5,6]
nTotal = 6
accuracyList = list()
globalOpt = []

for nFeature in range(1,nTotal):
    combinList = GenerateCombinations(index.copy(), 1)
    maxAccu = -np.inf
    testAccu = 0
    nameOpt_Combin = []
    
    for combin in combinList:
        #train_sample
        combinName = globalOpt + [featureNames[i] for i in combin]
        train_X = trainSample[combinName]
        test_X = testSample[combinName]       
        
        #Calculate LDA accuracy
        myLDA=LDA()
        myLDA.fit(train_X, train_Y.astype(int))
        predictTrain_Y = myLDA.predict(train_X)
        predictTest_Y = myLDA.predict(test_X)
        accuTrain = metrics.accuracy_score(train_Y,predictTrain_Y)
        accuTest = metrics.accuracy_score(test_Y,predictTest_Y)

        if accuTrain>maxAccu:
            maxAccu = accuTrain
            testAccu = accuTest
            optCombin = combin
            nameOpt_Combin = combinName
            
        elif accuTrain == maxAccu and sum(optCombin)>sum(combin):
            optCombin=combin
            nameOpt_Combin=combinName
           
    accuracyList.append([nFeature,maxAccu,testAccu, nameOpt_Combin])
    optIndex=index.index(optCombin[0])
    globalOpt.append(featureNames[index.pop(optIndex)])
    
    print(globalOpt)
    
print(accuracyList)  



 
